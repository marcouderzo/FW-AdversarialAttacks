{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **FZCGS**: Faster Zeroth-Order Conditional Gradient Sliding Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_pcSeubnRRK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Input(shape=(3,)),  # Input layer\n",
        "    keras.layers.Dense(64, activation='relu'),  # Hidden layer with 64 neurons and ReLU activation\n",
        "    keras.layers.Dense(3, activation='softmax')  # Output layer with softmax activation\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = None #x_0 in input to FZCGS\n",
        "n = 5 # number of component functions?\n",
        "\n",
        "OMEGA = [] # this is the R^d convex feasible set\n",
        "\n",
        "K = 1000\n",
        "L = None\n",
        "\n",
        "S1 = random.sample(x, n)\n",
        "S2 = random.sample(x, np.sqrt(n))\n",
        "\n",
        "\n",
        "d = None # what is d?\n",
        "\n",
        "q = len(S2)\n",
        "\n",
        "mu= 1 / np.sqrt(d*K)\n",
        "gamma = 1/3*L\n",
        "eta = 1/K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Coordinate-Wise Gradient Estimator and Conditional Gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def f(x):\n",
        "    return model.predict(x)\n",
        "\n",
        "def estimate_gradient(x): #mu smoothing parameter, e_j (R^d) basis vector where only j-th element is 1, otherwise 0.\n",
        "    grad = np.zeros(x.shape)\n",
        "    for j in range(1, d):\n",
        "        e = np.zeros(x.shape)\n",
        "        e[j] = 1  # Set the j-th element of the basis vector to 1 \n",
        "        part = ((f(x + mu*e) - f(x - mu*e)) / (2*mu)) * e\n",
        "        grad = grad + part\n",
        "    return grad\n",
        "\n",
        "\n",
        "\n",
        "def V(g: np.array, u: np.array, gamma, u_t: np.array) -> np.array:\n",
        "    dotp = []\n",
        "    for x in OMEGA: #original convex set\n",
        "        dotp.append((np.inner(g + (1/gamma)*(u_t - u), u_t - x)))\n",
        "    return np.max(dotp)\n",
        "\n",
        "\n",
        "def condg(g: np.array, u: np.array, gamma, eta):\n",
        "    t = 1\n",
        "\n",
        "    u_t= u\n",
        "    while(True):\n",
        "        v_t = V(g, u, gamma, u_t)\n",
        "\n",
        "        if v_t <= eta:  # v_t is indeed the Frank-Wolfe gap.\n",
        "            break\n",
        "        \n",
        "        norm = np.linalg.norm(v_t - u_t, ord=1)**2\n",
        "        arg = np.inner((1/gamma) * (u-u_t) - g, v_t-u_t) / ((1/gamma) * norm)\n",
        "        alpha_t = np.min([1, arg])\n",
        "        u_t = (1-alpha_t) * u[t] + alpha_t * v_t\n",
        "\n",
        "        t = t+1\n",
        "\n",
        "    return u_t "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def FZCGS(x_0, q, mu, K, eta, gamma, n):\n",
        "    v_k = np.array()\n",
        "    x_k = x_0\n",
        "\n",
        "    for k in range(0, K-1):\n",
        "        v_k_prev = v_k # store previous v_k\n",
        "        x_k_prev = x_k # store previous x_k\n",
        "        if np.mod(k, q) == 0: \n",
        "            v_k = estimate_gradient(x_k)\n",
        "        else:\n",
        "            g = estimate_gradient(x_k) \n",
        "            g_prev = estimate_gradient(x_k_prev)\n",
        "            v_k = (1/len(S2)) * np.sum((g - g_prev) + v_k_prev)\n",
        "        \n",
        "        x_k = condg(v_k, x_k, gamma, eta)\n",
        "    \n",
        "    return random.sample(x_k, 1)\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
