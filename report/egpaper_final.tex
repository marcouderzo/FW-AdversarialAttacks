\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Zeroth-Order Frank-Wolfe Optimization for Black-Box Adversarial Attacks}

\author{Marco Uderzo  \\
{\tt\small marco.uderzo@studenti.unipd.it}\\ 
\tt \small Student ID: 2096998
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   The goal of this project is to compare the behaviour and performance
   of two Zeroth-Order variants of the Frank-Wolfe Algorithm, aimed at 
   solving constrained optimization problems with a better iteration complexity,
   expecially with respect to oracle queries.
   We take into consideration: Faster Zeroth-Order Conditional Gradient Sliding (FZCGS)
   (Gao et al., 2018) and Stochastic Gradient Free Frank Wolfe 
   (SGFFW) (Sahu et al., 2019). The latter algorithm branches off into three slightly different ones,
   depending on the Stochastic Approximation Technique used, namely: classical Kiefer-Wolfowitz
   Stochastic Approximation (KWSA) (Kiefer and Wolfowitz, 1952), Random Directions Stochastic Approximation
   (RDSA) (Nesterov and Spokoiny, 2011; Duchi et al., 2015), and an Improvised RDSA (IRDSA). 
   The theory behind these algorithms is presented, with an emphasis on proving that the performance are guaranteed. 
   Then, the aforementioned algorithms are tested on a black-box adversarial attack on the MNIST dataset. 

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

The Frank-Wolfe algorithm, also known as the conditional gradient method, 
is an iterative optimization technique used for constrained convex optimization problems. 
It was proposed by Marguerite Frank and Philip Wolfe in 1956, and nowadays finds various applications
in the field of machine learning. It approximates the objective function by a first-order Taylor approximation.
The algorithm iteratively selects a direction that minimizes the linear approximation of the objective function
within the feasible set C. This direction is then combined with the current solution in a convex combination, and the
process is repeated until convergence.

In particular, the Frank-Wolfe algorithm excels in constrained optimization problems with a closed convex set C:

\[
\min _{x \in C} f(x)
\]

The problem formulation can vary widely; for example, Gao et al. deal with a variant tailored for finite-sum minimization problems, in which
the component functions $ f_{i}(x) $ are summed up as follows:

\[
\min _{x \in \Omega} F(x)=\frac{1}{n} \sum_{i=1}^{n} f_{i}(x)
\]

Sahu et al., on the other hand, in order to estimate the loss function f(x), deal with a variant that uses a stochastic zeroth-order oracle.
The loss function is therefore defined as:

\[
\min _{x \in C} f(x)=\min _{x \in C} \mathbb{E}_{y}[F(x, y)]
\]

This paper addresses the use of the Frank-Wolfe algorithm for a particularly critical constrained optimization problem in Deep Learning, which is the
problem of Adversarial Attacks. The objective of an adversarial attack is to find a small enough perturbation of the input able to make the neural network
output the wrong prediction, while adhering to constraints inside of the convex set C. In our case, we use MNIST, so the goal is to find a non-trivial perturbation of the 28x28 black and white image of a hand-written
digit. Moreover, as we will see later, we employ a zeroth-order variant of the Frank-Wolfe algorithm, since
we don't have access to the full exact gradient.


\subsection{Deterministic Frank-Wolfe Algorithm}

In case first-order information is available in an optimization task, the deterministic version of the Frank-Wolfe algorithm
can be a good choice, expecially when exact minimization is computationally expensive.

The exact minimization in the first formula in the introduction is approximated through an inexact minimization,
where a vector v satisfies some conditions, while maintaining the same convergence rate. \\

When full, exact first-order information is available through an incremental first-order oracle (IFO),
the Frank-Wolfe algorithm is basically described by the following two formulas:

\[
\begin{aligned}
v_{t} & =\arg \min _{v \in \mathcal{C}}\left\langle h, \nabla f\left(x_{t}\right)\right\rangle \\
x_{t+1} & =\left(1-\gamma_{t+1}\right) x_{t}+\gamma_{t+1} v_{t},
\end{aligned}
\]

where 

\begin{itemize}

   \item $f\left(x_{t}\right)$ is the objective function we need to minimize;

   \item $\mathcal{C}$ is the convex set;

   \item $\langle\cdot, \cdot\rangle$ is the inner/dot product;

   \item $v_{t}$ is the direction we need to take in order to minimize the linear approximation;

   \item $x_{t}$ is the current iteration result;

   \item $h$ is a vector in the same space as $x_{t}$;

   \item $\gamma_{t+1}=\frac{2}{t+2}$ is the step size.

\end{itemize}

\subsection{Stochastic Frank-Wolfe Algorithm}

In case first-order information is not available, and we can only work with zeroth-order information,
the stochastic variant of the Frank-Wolfe algorithm can be a good choice. \\

By employing a Stochastic Zeroth-order Oracle (SZO), the deterministic objective function is substituted by a 
stochastic objective function $f\left(x_{t}, y_{t}\right)$, with  $y_{t}$ being a random variable.
Therefore, the Stochastic Frank-Wolfe algorithm becomes:

\[
\begin{aligned}
v_{t} & =\arg \min _{v \in \mathcal{C}}\left\langle h, \nabla f\left(x_{t}, y_{t}\right)\right\rangle \\
x_{t+1} & =\left(1-\gamma_{t+1}\right) x_{t}+\gamma_{t+1} v_{t},
\end{aligned}
\]

\subsection{Zeroth-Order Gradient Estimation}

When the gradient of a function is unavailable, it is possible to estimate it using function evaluations,
by calculating the function values at selected points. More in detail, we can use the difference of the
function value with respect to two random points to estimate the gradient. In our case, we employ the use of
the coordinate-wise gradient estimator, as in the Gao et al. paper.

The coordinate-wise gradient estimator is defined as follows:

\[
\hat{\nabla} f(\mathbf{x})=\sum_{j=1}^{d} \frac{f\left(\mathbf{x}+\mu_{j} \mathbf{e}_{j}\right)-f\left(\mathbf{x}-\mu_{j} \mathbf{e}_{j}\right)}{2 \mu_{j}} \mathbf{e}_{j}
\]

where

\begin{itemize}



\item $\hat{\nabla} f(\mathbf{x})$ is the estimated gradient of the function $f$ in $\mathbf{x}$

\item $d$ is the dimensionality of the optimization space

\item $\mu_{j}>0$ is a smoothing parameter

\item $\mathbf{e}_{j} \in \mathbb{R}^{d}$ is the basis vector where only the j-th element is 1 and all others are 0.

\end{itemize}

\section{Implemented Algorithms}

This project involves the implementation of the following algorithms:

\begin{itemize}
   \item \textbf{FZCGS}: Faster Zeroth-Order Conditional Gradient Sliding Method
   \item \textbf{SGFFW}: Stochastic Gradient-Free Frank-Wolfe with the following gradient approximation schemes:
      \begin{itemize}
         \item \textbf{KWSA}: Kiefer-Wolfowitz stochastic approximation
         \item \textbf{RDSA}: random directions stochastic approximation
         \item \textbf{I-RDSA}: improvised random directions stochastic approximation
      \end{itemize}
\end{itemize}

\subsection{FZCGS: Faster Zeroth-Order Conditional Gradient Sliding Method}

Gao et al. proposes a novel algorithm to optimize the following constrained finite-sum minimization problem:

\[
\min _{x \in \Omega} F(x)=\frac{1}{n} \sum_{i=1}^{n} f_{i}(x)
\]

where

\begin{itemize}

\item $\Omega \subset \mathbb{R}^{d}$ is the closed convex feasible set
\item $f_{i}(x)$ are the various $n$ component functions, which are smooth and non-convex
\end{itemize}

%With FZCGS using a Function Query Oracle (FQO), the authors of the paper achieved a  
%$O\left(\frac{n^{1 / 2} d}{\epsilon^{2}}\right)$ oracle complexity.

FZCGS incorporates an acceleration technique from the non-convex Frank-Wolfe method. (?)
It uses gradient estimation and Conditional Gradient Sliding to perform the updates.
The convergence rate depends on the choice of the parameters. The pseudocode of the algorithm is presented below.

\includegraphics*[scale=0.7]{img/fzcgs_pseudocode.jpg} \\

\includegraphics*[scale=0.7]{img/condg_pseudocode.jpg} \\


The results of FZCGS from the paper are:

\begin{itemize}
   \item When choosing the right set of parameters for FZCGS, the expected squared norm of the gradient estimation error converges as per the given rate
   \item The amortized function queries oracle complexity is $O\left(\frac{n^{1 / 2} d}{\epsilon}\right)$.
   \item The linear oracle complexity is $O\left(\frac{1}{\epsilon^{2}}\right)$.
\end{itemize}



\section{References}

List and number all bibliographical references in 9-point Times,
single-spaced, at the end of your paper. When referenced in the text,
enclose the citation number in square brackets, for
example~\cite{Authors14}.  Where appropriate, include the name(s) of
editors of referenced books.



%-------------------------------------------------------------------------
\subsection{Illustrations, graphs, and photographs}

All graphics should be centered.  Please ensure that any point you wish to
make is resolvable in a printed copy of the paper.  Resize fonts in figures
to match the font in the body text, and choose line widths which render
effectively in print.  Many readers (and reviewers), even of an electronic
copy, will choose to print your paper in order to read it.  You cannot
insist that they do otherwise, and therefore must not assume that they can
zoom in to see tiny details on a graphic.

When placing figures in \LaTeX, it's almost always best to use
\verb+\includegraphics+, and to specify the  figure width as a multiple of
the line width as in the example below
{\small\begin{verbatim}
   \usepackage[dvips]{graphicx} ...
   \includegraphics[width=0.8\linewidth]
                   {myfile.eps}
\end{verbatim}
}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
