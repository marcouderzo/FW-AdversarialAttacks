\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Zeroth-Order Frank-Wolfe Optimization for Black-Box Adversarial Attacks}

\author{Marco Uderzo  \\
{\tt\small marco.uderzo@studenti.unipd.it}\\ 
\tt \small Student ID: 2096998
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   The goal of this project is to compare the behaviour and performance
   of two Zeroth-Order variants of the Frank-Wolfe Algorithm, aimed at 
   solving constrained optimization problems with a better iteration complexity,
   expecially with respect to oracle queries.
   We take into consideration: Faster Zeroth-Order Conditional Gradient Sliding (FZCGS)
   (Gao et al., 2018) and Stochastic Gradient Free Frank Wolfe 
   (SGFFW) (Sahu et al., 2019). The latter algorithm branches off into three slightly different ones,
   depending on the Stochastic Approximation Technique used, namely: classical Kiefer-Wolfowitz
   Stochastic Approximation (KWSA) (Kiefer and Wolfowitz, 1952), Random Directions Stochastic Approximation
   (RDSA) (Nesterov and Spokoiny, 2011; Duchi et al., 2015), and an Improvised RDSA (IRDSA). 
   The theory behind these algorithms is presented, with an emphasis on proving that the performance are guaranteed. 
   Then, the aforementioned algorithms are tested on a black-box adversarial attack on the MNIST dataset. 

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

The Frank-Wolfe algorithm, also known as the conditional gradient method, 
is an iterative optimization technique used for constrained convex optimization problems. 
It was proposed by Marguerite Frank and Philip Wolfe in 1956, and nowadays finds various applications
in the field of machine learning. It approximates the objective function by a first-order Taylor approximation.
The algorithm iteratively selects a direction that minimizes the linear approximation of the objective function
within the feasible set C. This direction is then combined with the current solution in a convex combination, and the
process is repeated until convergence.

In particular, the Frank-Wolfe algorithm excels in constrained optimization problems with a closed convex set C:

\[
\min _{x \in C} f(x)
\]

The problem formulation can vary widely; for example, Gao et al. deal with a variant tailored for finite-sum minimization problems, in which
the component functions $ f_{i}(x) $ are summed up as follows:

\[
\min _{x \in \Omega} F(x)=\frac{1}{n} \sum_{i=1}^{n} f_{i}(x)
\]

Sahu et al., on the other hand, in order to estimate the loss function f(x), deal with a variant that uses a stochastic zeroth-order oracle.
The loss function is therefore defined as:

\[
\min _{x \in C} f(x)=\min _{x \in C} \mathbb{E}_{y}[F(x, y)]
\]

This paper addresses the use of the Frank-Wolfe algorithm for a particularly critical constrained optimization problem in Deep Learning, which is the
problem of Adversarial Attacks. The objective of an adversarial attack is to find a small enough perturbation of the input able to make the neural network
output the wrong prediction, while adhering to constraints inside of the convex set C. In our case, we use MNIST, so the goal is to find a non-trivial perturbation of the 28x28 black and white image of a hand-written
digit. Moreover, as we will see later, we employ a zeroth-order variant of the Frank-Wolfe algorithm, since
we don't have access to the full exact gradient.


\subsection{Deterministic Frank-Wolfe Algorithm}

In case first-order information is available in an optimization task, the deterministic version of the Frank-Wolfe algorithm
can be a good choice, expecially when exact minimization is computationally expensive.

The exact minimization in the first formula in the introduction is approximated through an inexact minimization,
where a vector v satisfies some conditions, while maintaining the same convergence rate. \\

When full, exact first-order information is available through an incremental first-order oracle (IFO),
the Frank-Wolfe algorithm is basically described by the following two formulas:

\[
\begin{aligned}
v_{t} & =\arg \min _{v \in \mathcal{C}}\left\langle h, \nabla f\left(x_{t}\right)\right\rangle \\
x_{t+1} & =\left(1-\gamma_{t+1}\right) x_{t}+\gamma_{t+1} v_{t},
\end{aligned}
\]

where 

\begin{itemize}

   \item $f\left(x_{t}\right)$ is the objective function we need to minimize;

   \item $\mathcal{C}$ is the convex set;

   \item $\langle\cdot, \cdot\rangle$ is the inner/dot product;

   \item $v_{t}$ is the direction we need to take in order to minimize the linear approximation;

   \item $x_{t}$ is the current iteration result;

   \item $h$ is a vector in the same space as $x_{t}$;

   \item $\gamma_{t+1}=\frac{2}{t+2}$ is the step size.

\end{itemize}

\subsection{Stochastic Frank-Wolfe Algorithm}

In case first-order information is not available, and we can only work with zeroth-order information,
the stochastic variant of the Frank-Wolfe algorithm can be a good choice. \\

By employing a Stochastic Zeroth-order Oracle (SZO), the deterministic objective function is substituted by a 
stochastic objective function $f\left(x_{t}, y_{t}\right)$, with  $y_{t}$ being a random variable.
Therefore, the Stochastic Frank-Wolfe algorithm becomes:

\[
\begin{aligned}
v_{t} & =\arg \min _{v \in \mathcal{C}}\left\langle h, \nabla f\left(x_{t}, y_{t}\right)\right\rangle \\
x_{t+1} & =\left(1-\gamma_{t+1}\right) x_{t}+\gamma_{t+1} v_{t},
\end{aligned}
\]

\subsection{Zeroth-Order Gradient Estimation}

When the gradient of a function is unavailable, it is possible to estimate it using function evaluations,
by calculating the function values at selected points. More in detail, we can use the difference of the
function value with respect to two random points to estimate the gradient. In our case, we employ the use of
the coordinate-wise gradient estimator, as in the Gao et al. paper.

The coordinate-wise gradient estimator is defined as follows:

\[
\hat{\nabla} f(\mathbf{x})=\sum_{j=1}^{d} \frac{f\left(\mathbf{x}+\mu_{j} \mathbf{e}_{j}\right)-f\left(\mathbf{x}-\mu_{j} \mathbf{e}_{j}\right)}{2 \mu_{j}} \mathbf{e}_{j}
\]

where

\begin{itemize}



\item $\hat{\nabla} f(\mathbf{x})$ is the estimated gradient of the function $f$ in $\mathbf{x}$

\item $d$ is the dimensionality of the optimization space

\item $\mu_{j}>0$ is a smoothing parameter

\item $\mathbf{e}_{j} \in \mathbb{R}^{d}$ is the basis vector where only the j-th element is 1 and all others are 0.

\end{itemize}

\section{Implemented Algorithms}

This project involves the implementation of the following algorithms:

\begin{itemize}
   \item \textbf{FZCGS}: Faster Zeroth-Order Conditional Gradient Sliding Method
   \item \textbf{SGFFW}: Stochastic Gradient-Free Frank-Wolfe with the following gradient approximation schemes:
      \begin{itemize}
         \item \textbf{KWSA}: Kiefer-Wolfowitz stochastic approximation
         \item \textbf{RDSA}: random directions stochastic approximation
         \item \textbf{I-RDSA}: improvised random directions stochastic approximation
      \end{itemize}
\end{itemize}

\subsection{FZCGS: Faster Zeroth-Order Conditional Gradient Sliding Method}

Gao et al. proposes a novel algorithm to optimize the following constrained finite-sum minimization problem:

\[
\min _{x \in \Omega} F(x)=\frac{1}{n} \sum_{i=1}^{n} f_{i}(x)
\]

where

\begin{itemize}

\item $\Omega \subset \mathbb{R}^{d}$ is the closed convex feasible set
\item $f_{i}(x)$ are the various $n$ component functions, which are smooth and non-convex
\end{itemize}

%With FZCGS using a Function Query Oracle (FQO), the authors of the paper achieved a  
%$O\left(\frac{n^{1 / 2} d}{\epsilon^{2}}\right)$ oracle complexity.

FZCGS incorporates an acceleration technique from the non-convex Frank-Wolfe method. (?)
It uses gradient estimation and Conditional Gradient Sliding to perform the updates.
The convergence rate depends on the choice of the parameters. The pseudocode of the algorithm is presented below.

\includegraphics*[scale=0.7]{img/fzcgs_pseudocode.jpg} \\

\includegraphics*[scale=0.7]{img/condg_pseudocode.jpg} \\


The results of FZCGS from the paper are:

\begin{itemize}
   \item When choosing the right set of parameters for FZCGS, the expected squared norm of the gradient estimation error converges as per the given rate
   \item The amortized function queries oracle complexity is $O\left(\frac{n^{1 / 2} d}{\epsilon}\right)$.
   \item The linear oracle complexity is $O\left(\frac{1}{\epsilon^{2}}\right)$.
\end{itemize}

In the paper, the experimental results demonstate the superiority of FZCGS over baseline methods
in terms of convergence speed and performance, also thanks to the acceleration technique that this algorithm employs.

\subsection{Stochastic Gradient-Free Frank-Wolfe Algorithm}

Sahu et al. propose a stochastic zeroth-order Frank-Wolfe algorithm for the following stochastic
optimization problem, particularly significant in deep learning:

\[
\min _{x \in C} f(x)=\min _{x \in C} \mathbb{E}_{y}[F(x ; y)]
\]

where $C \in \mathbb{R}^{d}$ is a closed convex set.

To solve this kind of problem, one can employ projection and projection-free methods.
SGFFW uses a zeroth-order oracle while focusing on a projection-free stochastic variant
of Frank-Wolfe. It uses gradient approximation schemes and addresses challenges such as non-smoothness
and variance. Using an averaging trick to ensure the stability of the algorithm,
the updates are the following:

\[
\begin{aligned}
d_{t} & =\left(1-\rho_{t}\right) d_{t-1}+\rho_{t} g\left(x_{t}, y_{t}\right) \\
v_{t} & =\arg \min _{s \in C}\left\langle s, d_{t}\right\rangle \\
x_{t+1} & =\left(1-\gamma_{t+1}\right) x_{t}+\gamma_{t+1} v_{t}
\end{aligned}
\]

Employing a gradient approximation techniques or another influences the 
algorithm's dimension dependence and the computational cost, also in terms of query number and time.

\begin{itemize}
   \item {Kiefer-Wolfowitz Stochastic Approximation (KWSA):
   \[
   \mathbf{g}\left(\mathbf{x}_{t} ; \mathbf{y}\right)=\sum_{i=1}^{d} \frac{F\left(\mathbf{x}_{t}+c_{t} \mathbf{e}_{i} ; \mathbf{y}\right)-F\left(\mathbf{x}_{t} ; \mathbf{y}\right)}{c_{t}} \mathbf{e}_{i}
   \]}
   \item {Random Direction Stochastic Approximation (RDSA): \newline
   \text{Sample} $\mathbf{z}_{t} \sim \mathcal{N}\left(0, \mathbf{I}_{d}\right)$,
   \[
   \mathbf{g}\left(\mathbf{x}_{t} ; \mathbf{y}, \mathbf{z}_{t}\right)=\frac{F\left(\mathbf{x}_{t}+c_{t} \mathbf{z}_{t} ; \mathbf{y}\right)-F\left(\mathbf{x}_{t} ; \mathbf{y}\right)}{c_{t}} \mathbf{z}_{t}
   \]}

   \item {Improvised Random Direction Stochastic Approximation (I-RDSA): \newline
   \text {Sample}$\left\{\mathbf{z}_{i, t}\right\}_{i=1}^{m} \sim \mathcal{N}\left(0, \mathbf{I}_{d}\right)$ \newline
   \[
      \mathbf{g}\left(\mathbf{x}_{t} ; \mathbf{y}, \mathbf{z}_{t}\right)=\frac{1}{m} \sum_{i=1}^{m} \frac{F\left(\mathbf{x}_{t}+c_{t} \mathbf{z}_{i, t} ; \mathbf{y}\right)-F\left(\mathbf{x}_{t} ; \mathbf{y}\right)}{c_{t}} \mathbf{z}_{i, t}
   \]}
\end{itemize} \\

Below, the pseudocode of the algorithm is presented: \\

\begin{center}
   \includegraphics*[scale=0.75]{img/sgffw_pseudocode.png}
\end{center}

\subsubsection{Primal Gap}

Sahu et al. state the main results involving the different gradient 
approximation schemes for the primal (sub-optimality) gap, which provide a characterization of
\[E\left[f\left(x_{t}\right)-f\left(x^{*}\right)\right]\].

Given the sequence $\gamma_{t}= \frac{2}{t+8}$:

\begin{itemize}
   \item{RDSA gradient approximation scheme:

         \[
         E\left[f\left(x_{t}\right)-f\left(x^{*}\right)\right]=O\left(d^{1 / 3}(t+9)^{1 / 3}\right)
         \]
   }

   \item{I-RDSA gradient approximation scheme:
         \[
         E\left[f\left(x_{t}\right)-f\left(x^{*}\right)\right]=O\left(\frac{d}{m^{1 / 3}}(t+9)^{1 / 3}\right)
         \]
   }

   \item{KWSA gradient approximation scheme:
         \[
         E\left[f\left(x_{t}\right)-f\left(x^{*}\right)\right]=O\left(\frac{1}{(t+9)^{1 / 3}}\right)
         \]
   }
\end{itemize}


The dimension dependence of the primal gap is quantified to be
$d^{1 / 3}$. At the same time, the dependence on iterations
$O\left(T^{-1 / 3}\right)$, matches that of the stochastic 
Frank-Wolfe which has access to first-order information. 
The improvement of the rates for I-RDSA and KWSA are at the cost of
extra directional derivatives at each iteration.
The number of queries to the SZO to obtain a primal gap of $\epsilon$ is given by 
$O\left(\frac{d}{\epsilon^{3}}\right)$, where the dimension dependence
is consistent with zeroth-order schemes and cannot be improved on.

\subsubsection{Dual Gap}

The paper also quantifies the dual gap with the same gradient approximation schemes

\begin{itemize}
\item {RDSA gradient approximation scheme:

\[
\begin{aligned}
E\left[\min _{t=0, \ldots, T-1} G\left(x_{t}\right)\right] \leq & \frac{7\left(F\left(x_{0}\right)-F\left(x^{*}\right)\right)}{2 T} \\
& +\frac{L R^{2} \ln (T+7)}{T} \\
& +\frac{Q_{0}+R \sqrt{2 Q}}{2 T^{2 / 3}}
\end{aligned}
\]
}

\item {I-RDSA gradient approximation scheme:

\[
\begin{aligned}
E\left[\min _{t=0, \ldots, T-1} G\left(x_{t}\right)\right] \leq & \frac{7\left(F\left(x_{0}\right)-F\left(x^{*}\right)\right)}{2 T} \\
& +\frac{L R^{2} \ln (T+7)}{T} \\
& +\frac{Q_{i r}+R \sqrt{2 Q_{i r}}}{2 T^{2 / 3}}
\end{aligned}
\]
}

\item {KWSA gradient approximation scheme:

\[
\begin{aligned}
E\left[\min _{t=0, \ldots, T-1} G\left(x_{t}\right)\right] \leq & \frac{7\left(F\left(x_{0}\right)-F\left(x^{*}\right)\right)}{2 T} \\
& +\frac{L R^{2} \ln (T+7)}{T} \\
& +\frac{Q_{k w}+R \sqrt{2 Q_{k w}}}{2 T^{2 / 3}}
\end{aligned}
\]
}

\end{itemize}


The dimension dependence of the Frank-Wolfe duality gap is quantified to be $d^{1 / 3}$.
At the same time, the dependence on iterations, $O\left(T^{-1 / 3}\right)$, matches
that of the primal gap and hence follows that the number of queries to the SZO to obtain a Frank-Wolfe 
duality gap of $\epsilon$ is given by $O\left(\frac{d}{\epsilon^{3}}\right)$. 
In particular, it is also asserted that the initial conditions 
are forgotten as $O(1 / T)$.

\section{Frank-Wolfe for Adversarial Attacks on MNIST}

\subsection{Adversarial Attacks}

An Adversarial Attacks is a deliberate manipulation of input data with the 
intention of causing a machine learning model to make a mistake or produce 
incorrect outputs. The goal of an adversarial attack is to perturb 
the input in a way that is not easily noticeable to a human observer,
but can mislead the model into making errors. In particular, when it comes
to black-box attacks, the attacker has limited or no knowledge of the 
model's internal structure and parameters. These attacks can be performed on
deep neural networks models; in our case, the model is MNIST, designed for the
classification of handwritten digits. 
The MNIST dataset consists of 28x28 pixel grayscale images of handwritten digits (0 through 9).
The objective of our experiment is to compare the optimization of an adversarial attack using
zeroth-order Frank-Wolfe algorithms in a black-box setting.
The task is to find a non-trivial adversarial perturbation such that the DNN model
makes the incorrect prediction.

\subsection{Methods}

We follow the setup (Liu et al., 2019) with the nn-carlini pre-trained Deep Neural Network for
the MNIST dataset; this base repository is also used by Gao et al. for their experiments, so starting
from it as a base for the implementation of the optimization algorithms was deemed to be a good
way to set up our experiments as well. The base repository, as per Gao et al., can be found
at the following \href{https://github.com/IBM/ZOSVRG-BlackBox-Adv}{link}.

\subsubsection{Implementation Differences from Reference Papers}

For time and computational constraints, there are some implementative differences 
between the theoretical papers and our actual implementation.

\begin{itemize}
   \item In FZCGS the gradient is calculated over a set of 
   randomized directions, instead of over all 
   the directions built by the basis vectors; this is done because 
   while it is less precise over the same amount of epochs, 
   calculating the gradient over all basis vector directions 
   requires too many queries to the oracle and time per epoch.
   As of the current implementation, a 200 iterations-long 
   optimization run takes about around 40 hours to complete.

   \item In the \texttt{condg} (conditional gradient) step for FZCGS,
   we added an additional stopping condition on the size of alpha.
   This was done because the conditional gradient step, if not limited
   on iteration number, the stopping condition would not be reached even
   after 100000 loops. This, together with the extremely large query count
   that FZCGS requires, would have made infeasible the optimization process,
   because of time constraints. The newly added stopping condition speeds up
   the conditional gradient function running time, whilst causing negligible
   deviations from the original result.
\end{itemize}

\subsubsection{Feasible Sets}

The objective function used is the same from the base repository.
The algorithms were tested on two types of feasible sets:

\begin{itemize}
   \item For SGFFW (also FZCGS?), $\|\delta\|_{\infty} \leq s$ was used. This is 
   also the same constraint that Gao et al. references in section 4.3 for the formulation
   of the problem.
   \item For FZCGS, we used a set similar to a L1-Ball,  containing
   all the possible directions in which the sum of the elements is zero. All elements, except
   one, are equal to 2000/d, and their opposite.
\end{itemize}


\section{Results}

The general parameters (non algorithm-specific) for the setup are presented in the table below.

\begin{table}[h]
   \centering
   \begin{tabular}{ccc}
       \hline
       Parameter & Value & Description \\
       \hline
       nStage & 200 & Iterations\\
       \hline
       targetlabel & 4 & Target digit\\
       \hline
       nFunc & 10 & Imgs to attack simult.\\
       \hline
       const & 3 & Weight on attack loss\\
       \hline
       rvdist & UnitSphere & Random perturb. distr.\\ 
       \hline
   \end{tabular}
   \
   \caption{General Setup Parameters} 
   \label{tab:general_params}
\end{table}

\subsection{FZCGS}

The FZCGS algorithm presented in Gao et al. is evaluated with respect to 
loss against iteration count, query count and running time, as the below graphs show:

\begin{table}[h]
   \centering
   \begin{tabular}{ccc}
       \hline
       Parameter & Value & Description \\
       \hline
       nStage & 200 & Iterations\\
       \hline
       q & 3 & Batch size S2\\
       \hline
       K & 0.1 & K parameter\\
       \hline
       L & 50 & Lipschitz constant \\
       \hline
       mu & 0.11 & mu param in Gao\\
       \hline
       gamma & 0.01 & gamma param in Gao\\
       \hline
       eta & 0.1 & eta param in Gao\\
       \hline
   \end{tabular}
   \
   \caption{Parameters of the FZCGS Algorithm} 
   \label{tab:fzcgs_params}
\end{table}


\begin{center}
   \includegraphics*[scale=0.35]{img/FZCGS_loss_vs_iterations.png}
\end{center}

\begin{center}
   \includegraphics*[scale=0.35]{img/FZCGS_loss_vs_querycount.png}
\end{center}

\begin{center}
   \includegraphics*[scale=0.35]{img/FZCGS_loss_vs_runningtime.png}
\end{center}

It is clear from the graphs that 200 iterations are enough for FZCGS to converge to a good loss,
though, since it uses multiple stochastic examples at each epoch for each direction analyzed, 
performances considering query count and time are where FZCGS presents criticalities.
In fact, as we will discuss later, it is the worst algorithm in terms of running time between the ones
considered in this study. 

Moreover, we can see that after around 50 iterations, the algorithm stops 
improving in regards of the overall loss. This has to be attributed to the \texttt{Loss Attack} dropping
to zero, and this makes FZCGS converge to a higher loss compared to other SGFFW variants, as we show in the next section.
The following graph shows this behaviour:

\begin{center}
   \includegraphics*[scale=0.45]{img/FZCGS_Losses.png}
\end{center}

Interestingly, FZCGS reaches the best loss in the whole run at the 16th iteration (out of 200), though
we regard it as an outlier value, since the loss for the rest of the run stays over the best loss.



\subsection{SGFFW}

The SGFFW algorithm from Sahu et al. was evaluated with the three
gradient approximation schemes (RDSA, I-RDSA, KWSA).
Below, the comparison between these variants is presented,
comparing the overall losses against iteration count, query count and running time.


\begin{table}[h]
   \centering
   \begin{tabular}{ccc}
       \hline
       Parameter & Value & Description \\
       \hline
       nStage & 200 & Iterations\\
       \hline
       gamma & $\frac{2}{t+8}$ & gamma param \\
       \hline
       m & 50 & num of random vectors\\
       \hline
       s (RDSA) & 8 & s for RDSA\\
       s (I-RDSA) & 4 & s for I-RDSA\\
       s (I-RDSA) & 4 & s for KWSA\\
       \hline
   \end{tabular}
   \
   \caption{Parameters of the SGFFW Algorithm} 
   \label{tab:sgffw_params}
\end{table}


\begin{center}
   \includegraphics*[scale=0.35]{img/SGFFW_loss_vs_iterations.png}
\end{center}

\begin{center}
   \includegraphics*[scale=0.35]{img/SGFFW_loss_vs_querycount.png}
\end{center}

\begin{center}
   \includegraphics*[scale=0.35]{img/SGFFW_loss_vs_runningtime.png}
\end{center}

SGFFW has different pretty different behaviours when the three gradient approximation schemes
are compared.

The RDSA gradient approximation scheme is the fastest between all other 
in terms of time, but requires many more iterations to reach a comparable loss value.
Though, the query count of RDSA is very limited, which is beneficial for the running time.\\

I-RDSA requires less iterations to reach a similar loss to RDSA, at the expense to a bigger query
count and a larger running time. \\

Between gradient approximation schemes for SGFFW, KWSA is the worst in terms of time, requiring
a very considerable amount of time to reach a good loss, while performing the least amount of iterations.
Other performed tests show that increasing iterations does not yield better results. \\

Considering iterations only, RDSA requires the most of them and therefore is the worst algorithm, 
but since the query count requirement for each iteration is very low, it is the fastest. \\

Since this study focuses more on query count performance, whereas iteration count is also taken into account
in other papers, we can say that overall I-RDSA performs best between the three approximation schemes for SGFFW,
providing low query count, low iteration requirements and a reasonable running time to converge.


\subsection{All Algorithms Compared}

In this section, we are going to compare all the algorithms and determine which performing best,
taking into account the best loss achieved, the last loss computed, 
the iteration count, the query count and running time, measured in hours.

\begin{table}[h]
   \centering
   \begin{tabular}{ccc}
       \hline
       Algorithm & Metric & Value \\
       \hline
       FZCGS & Best Loss & 12.689 @ 16th it.\\
            & Last Loss & 16.9\\
            & Iteration Count & 200\\
            & Query Count & 2198768\\
            & RunTime (Hrs) & 39.4\\
       \hline
       SGFFW-RDSA & Best Loss & 16.9 @ 5788th it.\\
                  & Last Loss & 18.922\\
                  & Iteration Count & 6000\\
                  & Query Count & 12000\\
                  & RunTime (Hrs) & 0.43\\
       \hline
       SGFFW-I-RDSA & Best Loss & 8.96 @ 245th it.\\
                    & Last Loss & 12.156\\
                    & Iteration Count & 500\\
                    & Query Count & 55000\\
                    & RunTime (Hrs) & 1.3\\
       \hline
       SGFFW-KWSA & Best Loss & 10.647 @ 199th it.\\
                  & Last Loss & 10.647\\
                  & Iteration Count & 200\\
                  & Query Count & 313600\\
                  & RunTime (Hrs) & 7.7\\
       \hline
   \end{tabular}
   \
   \caption{Overall results and statistics compared} 
   \label{tab:overall_results}
\end{table}

Notably, we see that I-RDSA converged to the best loss, followed by KWSA and FZCGS.
RDSA, while being the fastest, converged to the worst loss between all algorithms.

Below, the result graphs are presented. Each graph is also replotted
while limiting the x-axis, to better appreciate the behaviour of each algorithm.

\subsubsection*{Losses vs Iterations (All)}

\begin{center}
   \includegraphics*[scale=0.35]{img/All_loss_vs_iterations.png}
\end{center}
\begin{center}
   \includegraphics*[scale=0.35]{img/xLimit_All_loss_vs_iterations.png}\\
\end{center} 

\subsubsection*{Losses vs Query Count (All)}

\begin{center}
   \includegraphics*[scale=0.35]{img/All_loss_vs_querycount.png}
\end{center}
\begin{center}
   \includegraphics*[scale=0.35]{img/xLimit_All_loss_vs_querycount.png}\\
\end{center} 

\subsubsection*{Losses vs Running Time in Hours (All)}

\begin{center}
   \includegraphics*[scale=0.35]{img/All_loss_vs_runningtime.png}
\end{center}
\begin{center}
   \includegraphics*[scale=0.35]{img/xLimit_All_loss_vs_runningtime.png}\\
\end{center}



\section{Conclusion}

Overall, there is no overall best method, and each one has its own strengths and weaknesses. \\

From our study, FZCGS and SGFFW with KWSA had the best performance when considering iterations;
however, when compared to all of the SGFFW variants, they are the worst in terms
of query count and running time, making them less favourable.

SGFFW with RDSA is the fastest for running time, but having the
worst iterations count required to converge and reaching the highest loss among all algorithms considered. \\

Nonetheless, SGFFW with I-RDSA gradient approximation scheme showed a remarkable
compromise between running time, iteration count and query count, making it the 'best' overall method
for zeroth-order Frank-Wolfe optimization in the setting of adversarial attacks against the MNIST dataset.



\section{Code Availability}

All the code, data and results are accessible at the following \href{https://github.com/marcouderzo/FW-AdversarialAttacks}{link}.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
